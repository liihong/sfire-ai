"""
ç«æºæ–‡æ¡ˆæ™ºèƒ½ä½“ - Backend API

FastAPI application providing LLM-powered content generation services.
"""

import os
from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv

from services.llm_service import LLMFactory

# Load environment variables
load_dotenv()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    print("ğŸš€ ç«æºæ–‡æ¡ˆæ™ºèƒ½ä½“ Backend starting...")
    print(f"ğŸ“¦ Supported LLM models: {LLMFactory.get_supported_models()}")
    yield
    # Shutdown
    print("ğŸ‘‹ Backend shutting down...")


app = FastAPI(
    title="ç«æºæ–‡æ¡ˆæ™ºèƒ½ä½“ API",
    description="AI-powered content generation backend service",
    version="1.0.0",
    lifespan=lifespan
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============== Request/Response Models ==============

class GenerateRequest(BaseModel):
    """Request model for text generation."""
    prompt: str = Field(..., description="The input prompt for generation")
    model_type: str = Field(
        default="deepseek",
        description="LLM model type: 'deepseek' or 'doubao'"
    )
    system_prompt: Optional[str] = Field(
        default=None,
        description="Optional system prompt to set context"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="Sampling temperature (0.0-2.0)"
    )
    max_tokens: int = Field(
        default=2048,
        ge=1,
        le=8192,
        description="Maximum tokens to generate"
    )
    stream: bool = Field(
        default=False,
        description="Enable streaming response"
    )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "prompt": "å†™ä¸€æ®µå…³äºAIæ•°å­—äººçš„è¥é”€æ–‡æ¡ˆ",
                    "model_type": "deepseek",
                    "temperature": 0.7,
                    "max_tokens": 1024,
                    "stream": False
                }
            ]
        }
    }


class GenerateResponse(BaseModel):
    """Response model for text generation."""
    success: bool = Field(..., description="Whether the request was successful")
    content: str = Field(..., description="Generated content")
    model_type: str = Field(..., description="The LLM model used")
    usage: Optional[dict] = Field(default=None, description="Token usage statistics")


class ErrorResponse(BaseModel):
    """Error response model."""
    success: bool = False
    error: str = Field(..., description="Error message")
    detail: Optional[str] = Field(default=None, description="Detailed error information")


# ============== API Endpoints ==============

@app.get("/")
async def root():
    """Root endpoint - API health check."""
    return {
        "service": "ç«æºæ–‡æ¡ˆæ™ºèƒ½ä½“ API",
        "status": "running",
        "version": "1.0.0",
        "supported_models": LLMFactory.get_supported_models()
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy"}


@app.get("/api/models")
async def get_supported_models():
    """Get list of supported LLM models."""
    return {
        "models": LLMFactory.get_supported_models(),
        "default": "deepseek"
    }


@app.post(
    "/api/generate",
    response_model=GenerateResponse,
    responses={
        400: {"model": ErrorResponse},
        500: {"model": ErrorResponse}
    }
)
async def generate_content(request: GenerateRequest):
    """
    Generate content using the specified LLM model.
    
    This endpoint supports both DeepSeek and Doubao (ç«å±±å¼•æ“) models.
    
    - **prompt**: The input text for generation
    - **model_type**: Choose 'deepseek' or 'doubao'
    - **system_prompt**: Optional system context
    - **temperature**: Controls randomness (0.0-2.0)
    - **max_tokens**: Maximum length of generated content
    - **stream**: Enable streaming response (returns SSE)
    """
    try:
        # Validate model type
        supported_models = LLMFactory.get_supported_models()
        if request.model_type.lower() not in supported_models:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported model type: '{request.model_type}'. "
                       f"Supported: {supported_models}"
            )
        
        # Create LLM instance using factory
        llm = LLMFactory.create(request.model_type)
        
        # Handle streaming response
        if request.stream:
            async def generate_stream():
                async for chunk in llm.generate_stream(
                    prompt=request.prompt,
                    system_prompt=request.system_prompt,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens
                ):
                    yield f"data: {chunk}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
        
        # Non-streaming response
        content = await llm.generate_text(
            prompt=request.prompt,
            system_prompt=request.system_prompt,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        return GenerateResponse(
            success=True,
            content=content,
            model_type=request.model_type
        )
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Generation failed: {str(e)}"
        )


# ============== Content Generation Shortcuts ==============

@app.post("/api/generate/copywriting")
async def generate_copywriting(
    topic: str = Query(..., description="æ–‡æ¡ˆä¸»é¢˜"),
    style: str = Query(default="è¥é”€", description="æ–‡æ¡ˆé£æ ¼ï¼šè¥é”€/ç§è‰/ç§‘æ™®/æ•…äº‹"),
    model_type: str = Query(default="deepseek", description="æ¨¡å‹ç±»å‹"),
    max_tokens: int = Query(default=1024, description="æœ€å¤§é•¿åº¦")
):
    """
    Generate marketing copywriting for the given topic.
    
    Specialized endpoint for content creation.
    """
    system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡ˆåˆ›ä½œä¸“å®¶ï¼Œæ“…é•¿{style}ç±»å‹çš„å†…å®¹åˆ›ä½œã€‚
è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸»é¢˜ï¼Œåˆ›ä½œä¸€æ®µå¸å¼•äººçš„æ–‡æ¡ˆã€‚
è¦æ±‚ï¼š
1. å†…å®¹è¦æœ‰å¸å¼•åŠ›å’Œæ„ŸæŸ“åŠ›
2. è¯­è¨€æµç•…è‡ªç„¶
3. é€‚åˆåœ¨ç¤¾äº¤åª’ä½“ä¼ æ’­
4. åŒ…å«é€‚å½“çš„æƒ…æ„Ÿè¡¨è¾¾"""
    
    prompt = f"è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜åˆ›ä½œä¸€æ®µ{style}æ–‡æ¡ˆï¼š\n\nä¸»é¢˜ï¼š{topic}"
    
    request = GenerateRequest(
        prompt=prompt,
        model_type=model_type,
        system_prompt=system_prompt,
        max_tokens=max_tokens
    )
    
    return await generate_content(request)


@app.post("/api/generate/script")
async def generate_script(
    topic: str = Query(..., description="è§†é¢‘ä¸»é¢˜"),
    duration: str = Query(default="60ç§’", description="è§†é¢‘æ—¶é•¿ï¼š30ç§’/60ç§’/3åˆ†é’Ÿ"),
    model_type: str = Query(default="deepseek", description="æ¨¡å‹ç±»å‹"),
    max_tokens: int = Query(default=2048, description="æœ€å¤§é•¿åº¦")
):
    """
    Generate video script for digital human.
    
    Creates structured scripts suitable for AI digital human videos.
    """
    system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çŸ­è§†é¢‘è„šæœ¬åˆ›ä½œä¸“å®¶ã€‚
è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸»é¢˜ï¼Œåˆ›ä½œä¸€ä¸ªé€‚åˆ{duration}çš„å£æ’­è„šæœ¬ã€‚
è¦æ±‚ï¼š
1. å¼€å¤´è¦æœ‰å¸å¼•åŠ›çš„hook
2. å†…å®¹ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æµç•…
3. è¯­è¨€é€‚åˆå£æ’­ï¼Œè‡ªç„¶äº²åˆ‡
4. ç»“å°¾è¦æœ‰æ˜ç¡®çš„è¡ŒåŠ¨å·å¬ï¼ˆCTAï¼‰
5. æ ‡æ³¨é€‚å½“çš„æƒ…æ„Ÿå’ŒèŠ‚å¥æç¤º"""
    
    prompt = f"è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜åˆ›ä½œä¸€ä¸ª{duration}çš„å£æ’­è§†é¢‘è„šæœ¬ï¼š\n\nä¸»é¢˜ï¼š{topic}"
    
    request = GenerateRequest(
        prompt=prompt,
        model_type=model_type,
        system_prompt=system_prompt,
        max_tokens=max_tokens
    )
    
    return await generate_content(request)


# ============== Run Server ==============

if __name__ == "__main__":
    import uvicorn
    
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=True,
        log_level="info"
    )


